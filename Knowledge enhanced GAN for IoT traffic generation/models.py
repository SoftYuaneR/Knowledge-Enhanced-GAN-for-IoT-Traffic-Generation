import os, sys
import time
import copy
import matplotlib
import random
import torch
import math
import warnings
import torch.nn as nn
from torch.nn.utils import weight_norm
import torch.nn.functional as F
from torch.autograd import Variable
from torch.nn.parameter import Parameter # import Parameter to create custom activations with learnable parameters
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.autograd as autograd
import numpy as np
from scipy.spatial import distance
from matplotlib import pyplot as plt
from tqdm import trange
import setproctitle  
setproctitle.setproctitle("traffic_gene@hsd")

USE_KGE = True

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor
		
def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

def _get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])

class TransBlock(nn.Module):
    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation="relu", num_layers=3, norm=None):
        super(TransBlock, self).__init__()
        self.layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)
        self.layers = _get_clones(self.layer, num_layers)
        self.num_layers = num_layers
        self.norm = norm
        self.apply(self._init_weights)
		
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
#         elif isinstance(m, nn.Conv2d):
#             trunc_normal_(m.weight, std=.02)
#             if isinstance(m, nn.Conv2d) and m.bias is not None:
#                 nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        
    def forward(self, x):
        for lyr in self.layers:
            x = lyr(x)
        if self.norm is not None:
            x = self.norm(x)
        return x

class PositionalEncoding_old(nn.Module):
    def __init__(self, d_model, dropout, max_len=300):
        """
        :param d_model: pe编码维度，一般与word embedding相同，方便相加 (our's user_num)
        :param dropout: dorp out
        :param max_len: 语料库中最长句子的长度，即word embedding中的L
        """
        super(PositionalEncoding_old, self).__init__()
        # 定义drop out
        self.dropout = nn.Dropout(p=dropout)

        # 计算pe编码
        pe = torch.zeros(max_len, d_model) # 建立空表，每行代表一个词的位置，每列代表一个编码位
        position = torch.arange(0, max_len).unsqueeze(1) # 建个arrange表示词的位置以便公式计算，size=(max_len,1)
        div_term = torch.exp(torch.arange(0, d_model, 2) *    # 计算公式中10000**（2i/d_model)
                             -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)  # 计算偶数维度的pe值
        pe[:, 1::2] = torch.cos(position * div_term)  # 计算奇数维度的pe值
        pe = pe.unsqueeze(0)  # size=(1, L, d_model)，为了后续与word_embedding相加,意为batch维度下的操作相同
        self.register_buffer('pe', pe)  # pe值是不参加训练的

    def forward(self, x): # x:(BZ, user_num, traffic_len)
        # 输入的最终编码 = cat(word_embedding, positional_embedding)
        x = x.permute(0,2,1) #(BZ, traffic_len, user_num)
        #print(x.shape, x.size(1), Variable(self.pe[:, :x.size(1)],requires_grad=False).shape)
        #x = torch.cat((x, Variable(self.pe[:, :x.size(1)],requires_grad=False).expand(x.shape)), 2) #size = (BZ, traffic_len, user_num*2)
        x = x + Variable(self.pe[:, :x.size(1)],requires_grad=False) #size = (BZ, traffic_len, user_num)
        return self.dropout(x.permute(0,2,1)) # size = (BZ, user_num, traffic_len)

class TemporalEncoding(nn.Module):
    def __init__(self, d_model, dropout, max_len=672):
        """
        :param d_model: pe编码维度，一般与word embedding相同，方便相加
        :param dropout: dorp out
        :param max_len: 语料库中最长句子的长度，即word embedding中的L
        """
        super(TemporalEncoding, self).__init__()
        # 定义drop out
        self.dropout = nn.Dropout(p=dropout)

        # 计算pe编码
        pe = torch.zeros(max_len, d_model) # 建立空表，每行代表一个词的位置，每列代表一个编码位
        position = torch.arange(0, max_len, 0.25).reshape(-1, d_model)
        pe = position/position.max()
        pe = pe.unsqueeze(0)  # size=(1, L, d_model)，为了后续与word_embedding相加,意为batch维度下的操作相同
        self.register_buffer('pe', pe)  # pe值是不参加训练的

    def forward(self, x): 
        # 输入的最终编码 = cat(word_embedding, positional_embedding)
        x = x.permute(1,0,2)
        #print(x.shape, x.size(1), Variable(self.pe[:, :x.size(1)],requires_grad=False).shape)
        x = torch.cat((x, Variable(self.pe[:, :x.size(1)],requires_grad=False).expand(x.shape)), 2) #size = [batch, L, d_model*2]
        #x = x + Variable(self.pe[:, :x.size(1)],requires_grad=False) #size = [batch, L, d_model]
        return self.dropout(x.permute(1,0,2)) # size = [L, batch, d_model]

class MyDataset(Dataset):
    def __init__(self, data):
        self.data = np.load(data)
        try:
            self.utf = torch.from_numpy(self.data['utf'].astype(np.float32))
            self.utf_norm = torch.from_numpy(self.data['utf_norm'].astype(np.float32))
        except:
            print('load data error!')
        
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()        
        utf = self.utf[idx]
        utf_norm = self.utf_norm[idx]
        return utf, utf_norm
    def __len__(self):
        return self.utf_norm.shape[0] 

class Generator(nn.Module):
    def __init__(self, dmodel=[6, 6*6, 6*48], length=6*48, noise_size=6*6, nhead=1, dim_feedforward=2048, dropout=0.1, activation="relu", num_layers=6):
        super(Generator, self).__init__()
        self.dmodel = dmodel
        self.length = length
        self.emdim = int(length*length/dmodel[2])
        self.linear0 =  nn.Linear(noise_size, self.dmodel[0]*self.emdim)
        self.linear1 =  nn.Linear(self.dmodel[0], self.dmodel[1])
        self.linear2 =  nn.Linear(self.dmodel[1], self.dmodel[2])
        self.linears = nn.ModuleList([self.linear0, self.linear1, self.linear2])
        self.norms = nn.ModuleList([nn.LayerNorm(di)  for di in self.dmodel])
        
        #self.block_month = TransBlock(self.dmodel[i], 1, dim_feedforward, dropout, activation, num_layers)
        self.blocks = nn.ModuleList([ TransBlock(di, nhead, dim_feedforward, dropout, activation, num_layers) for di in self.dmodel])
        self.relu = nn.ReLU()
        #if SHAPE_M0 > 1:
        #    self.linear_out = nn.Linear(SHAPE_M0, 1)


    def forward(self, x):
        BZ = x.shape[0] # (BZ, in_dim)
        #x = self.linear(x)
        for index, blk in enumerate(self.blocks):
            x = self.linears[index](x).view((-1, BZ, self.dmodel[index]))
            x = blk(self.norms[index](x))
        #print(x.max(), x.min())
        #if SHAPE_M0 > 1:
        #    x = self.linear_out(x)      
        return self.relu(x).permute(1,0,2).reshape((BZ, -1, self.length)) # (BZ, user_num, traffic_len)

class Discriminator(nn.Module):
    def __init__(self,  dmodel=[6*48, 6*6, 6], length=6*48, nhead=1, dim_feedforward=2048, dropout=0.1, activation="relu", num_layers=6):
        super(Discriminator, self).__init__()
        self.dmodel = dmodel
        self.length = length
        self.emdim = int(length*length/dmodel[0])
        self.linear0 =  nn.Linear(self.dmodel[0], self.dmodel[1])
        self.linear1 =  nn.Linear(self.dmodel[1], self.dmodel[2])
        self.linear2 =  nn.Linear(self.dmodel[2], 1)
        self.linears = nn.ModuleList([self.linear0, self.linear1, self.linear2])
        self.norms = nn.ModuleList([nn.LayerNorm(di)  for di in dmodel[1:]] + [nn.LayerNorm(1)])
        self.linear_out =  nn.ModuleList([nn.Linear(self.emdim, int(self.emdim/2)), \
		                                  nn.LayerNorm(int(self.emdim/2)),\
										  nn.Linear(int(self.emdim/2), 1)])
        #self.block_month = TransBlock(self.dmodel[i], 1, dim_feedforward, dropout, activation, num_layers)
        self.blocks = nn.ModuleList([ TransBlock(di, nhead, dim_feedforward, dropout, activation, num_layers) for di in self.dmodel])
        self.sigmoid = nn.Sigmoid()
        self.pe = PositionalEncoding(self.emdim, dropout, max_len=dmodel[0])
        self.pes = nn.ModuleList([ self.pe, self.pe, self.pe ])

    def forward(self, x, k):        
        BZ = x.shape[0]
        x = x.view(BZ, self.emdim, self.dmodel[0])
        for index, blk in enumerate(self.blocks):
            x = self.pes[index](x)
            x = blk(x.permute(1,0,2))
            x = self.linears[index](x)
            x = self.norms[index](x).permute(1,0,2)
        #    xx = 
        x = x.view(BZ, -1)
        for index, lo in enumerate(self.linear_out):
            x = lo(x)
        x = self.sigmoid(x) 
        #x = self.relu(self.linear3(self.linear2(self.linear1(self.linear(torch.cat((x.permute(1,0,2).reshape(BZ, -1), kge), 1))))))     
        return x

class Chomp1d(nn.Module):
    def __init__(self, chomp_size):
        super(Chomp1d, self).__init__()
        self.chomp_size = int(chomp_size)

    def forward(self, x):
        return x.contiguous()
        #return x[:, :, :-self.chomp_size].contiguous()

class soft_exponential(nn.Module):
    '''
    Implementation of soft exponential activation.
    Shape:
        - Input: (N, *) where * means, any number of additional
          dimensions
        - Output: (N, *), same shape as the input
    Parameters:
        - alpha - trainable parameter
    References:
        - See related paper:
        https://arxiv.org/pdf/1602.01321.pdf
    Examples:
        >>> a1 = soft_exponential(256)
        >>> x = torch.randn(256)
        >>> x = a1(x)
    '''
    def __init__(self, in_features, alpha = None):
        '''
        Initialization.
        INPUT:
            - in_features: shape of the input
            - aplha: trainable parameter
            aplha is initialized with zero value by default
        '''
        super(soft_exponential,self).__init__()
        self.in_features = in_features

        # initialize alpha
        if alpha == None:
            self.alpha = Parameter(torch.tensor(0.0)) # create a tensor out of alpha
        else:
            self.alpha = Parameter(torch.tensor(alpha)) # create a tensor out of alpha
            
        self.alpha.requiresGrad = True # set requiresGrad to true!

    def forward(self, x):
        '''
        Forward pass of the function.
        Applies the function to the input elementwise.
        '''
        #return (torch.exp(x) - 1) + 1.0
        #print('soft_exponential', 'alpha', self.alpha)
        if (self.alpha == 0.0):
            return x

        if (self.alpha < 0.0):
            return - torch.log(1 - self.alpha * (x + self.alpha)) / self.alpha

        if (self.alpha > 0.0):
#            print('sssss', (torch.exp(self.alpha * x) - 1)/ self.alpha + self.alpha)
            return (torch.exp(self.alpha * x) - 1)/ self.alpha + self.alpha
            
class TemporalBlock(nn.Module):
    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2, padding_mode='circular'):
        super(TemporalBlock, self).__init__()
        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,
                                           stride=stride, padding=padding, padding_mode=padding_mode, dilation=dilation))
        self.chomp1 = Chomp1d(padding/2)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)

        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,
                                           stride=stride, padding=padding, padding_mode=padding_mode, dilation=dilation))
        self.chomp2 = Chomp1d(padding/2)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)

        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,
                                 self.conv2, self.chomp2, self.relu2, self.dropout2)
        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        self.relu = nn.ReLU()
        self.init_weights()

    def init_weights(self):
        self.conv1.weight.data.normal_(0, 0.01)
        self.conv2.weight.data.normal_(0, 0.01)
        if self.downsample is not None:
            self.downsample.weight.data.normal_(0, 0.01)

    def forward(self, x):
        #print('xx', x.shape)
        out = self.net(x)
        #out_0 = self.conv1(x)
        #out_1 = self.dropout1(self.relu1(self.chomp1(out_0)))
        #out_2 = self.conv2(out_1)
        #out = self.dropout2(self.relu2(self.chomp2(out_2)))
        index_2 = int((out.shape[2]-x.shape[2])/2)
        out = out[:,:,index_2:index_2+x.shape[2]]
        res = x if self.downsample is None else self.downsample(x)
        #print(x.shape, out.shape, self.downsample, res.shape)#out_0.shape, out_1.shape, out_2.shape, out.shape,
        return self.relu(out + res)


class TemporalConvNet(nn.Module):
    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2, dilation_size_list=''):
        super(TemporalConvNet, self).__init__()
        layers = []
        num_levels = len(num_channels)
        for i in range(num_levels):
            if dilation_size_list:
                dilation_size = dilation_size_list[i]
            else:
                dilation_size = 2 ** i
            in_channels = num_inputs if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            #print(i, dilation_size)
            dilation_size = int(288/kernel_size) if kernel_size*dilation_size>288 else dilation_size
            #print(i, dilation_size)
            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\
                                     #padding=kernel_size*dilation_size*2, dropout=dropout)]
                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        #print('xx', x.shape)
        return self.network(x)

class SumDiscriminator(nn.Module):
    def __init__(self, input_size, num_channels, dropout, kge_size=32, kge_squeeze_size=3):
        super(SumDiscriminator, self).__init__()
        self.tcn_d = TemporalConvNet(input_size, num_channels, kernel_size=48, dropout=dropout)
        self.tcn_w = TemporalConvNet(input_size, num_channels, kernel_size=48*6, dropout=dropout)
        #self.tcn_m = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size[2], dropout=dropout)
        self.linear = nn.Linear(num_channels[-1]*2+kge_squeeze_size, 1) if USE_KGE else \
                      nn.Linear(num_channels[-1]*2, 1)
        #self.linear_kge = nn.Linear(kge_size, kge_squeeze_size)
        self.sigmoid = nn.Sigmoid()
        self.init_weights()

    def init_weights(self):
        self.linear.weight.data.normal_(0, 0.01)

    def forward(self, x, kge):
        y_d = self.tcn_d(x)[:,:,-1]
        y_w = self.tcn_w(x)[:,:,-1]
        #y_m = self.tcn_m(x)[:,:,-1]
        #print(y_d.shape, kge.shape)
        y = self.linear(torch.cat((y_d, y_w, self.linear_kge(kge)), 1)) if USE_KGE else \
            self.linear(torch.cat((y_d, y_w), 1))
        return self.sigmoid(y)


class UserTrafficDataset(Dataset):
    def __init__(self, data='/data5/huishuodi/pythonProject4trafficGeneration/utf_dataset_file_20220103.npz', \
                 file_num=6055, dense=True, cluster=None, fine_grain=False ):
        self.data = np.load(data)
        try:
            if fine_grain:
                #self.dense_tag = self.data['dense_tag']
                self.user_id = self.data['user_id'][0:file_num]
                self.app_usage = torch.from_numpy(self.data['app_usage'][0:file_num].astype(np.float32))
                self.geo_sta = torch.from_numpy(self.data['geo_sta'][0:file_num].astype(np.float32))
                self.utf = torch.from_numpy(self.data['utf_norm_10min'][0:file_num].astype(np.float32))
                self.utf_norm = torch.from_numpy(self.data['utf_norm_10min'][0:file_num].astype(np.float32))
                if cluster == 'kmeans':
                    self.cluster_label = torch.tensor(self.data['Kclusters4dense'], dtype=torch.int64)
                elif cluster == 'spectral':
                    self.cluster_label = torch.tensor(self.data['Sclusters4dense'], dtype=torch.int64)
                else:
                    self.cluster_label = -1. * torch.zeros(len(self.user_id))
            elif dense:
                self.dense_tag = self.data['dense_tag']
                self.user_id = self.data['user_id'][self.dense_tag][0:file_num]
                self.app_usage = torch.from_numpy(
                    self.data['app_usage'][self.dense_tag][0:file_num].astype(np.float32))
                self.geo_sta = torch.from_numpy(
                    self.data['geo_sta'][self.dense_tag][0:file_num].astype(np.float32))
                self.utf = torch.from_numpy(self.data['utf'][self.dense_tag][0:file_num].astype(np.float32))
                self.utf_norm = torch.from_numpy(
                    self.data['utf_norm'][self.dense_tag][0:file_num].astype(np.float32))
                if cluster == 'kmeans':
                    self.cluster_label = torch.tensor(self.data['Kclusters4dense'], dtype=torch.int64)
                elif cluster == 'spectral':
                    self.cluster_label = torch.tensor(self.data['Sclusters4dense'], dtype=torch.int64)
                else:
                    self.cluster_label = -1. * torch.zeros(len(self.user_id))
            else:
                self.user_id = self.data['user_id'][0:file_num]
                self.app_usage = torch.from_numpy(self.data['app_usage'][0:file_num].astype(np.float32))
                self.geo_sta = torch.from_numpy(self.data['geo_sta'][0:file_num].astype(np.float32))
                self.utf = torch.from_numpy(self.data['utf'][0:file_num].astype(np.float32))
                self.utf_norm = torch.from_numpy(self.data['utf_norm'][0:file_num].astype(np.float32))
                self.cluster_label =  -1. * torch.zeros(len(self.user_id))

        except Exception as E:
            print(E)
            print('load data error!')

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        #utf = self.utf[idx]
        user_id = self.user_id[idx]
        app_usage = self.app_usage[idx]
        geo_sta = self.geo_sta[idx]
        utf_norm = self.utf_norm[idx]
        utf = self.utf[idx]
        cluster_label = self.cluster_label[idx]
        return user_id, app_usage, geo_sta, utf_norm, utf, cluster_label
        #return user_id, app_usage, geo_sta, utf_norm, utf

    def __len__(self):
        return len(self.user_id)

class MLPGenerator(nn.Module):
    def __init__(self, noise_size=32, out_seq_size=288, hidden_size=None, condition_size = 32):
        super(MLPGenerator, self).__init__()
        if hidden_size is None:
            hidden_size = [6 * 12, 6 * 24, 6 * 48]
        hidden_size.append(out_seq_size)
        self.linear_in = nn.Linear(noise_size+condition_size, hidden_size[0]) if USE_KGE else \
                         nn.Linear(noise_size, hidden_size[0])
        self.linear_list = nn.ModuleList([self.linear_in, nn.LayerNorm(hidden_size[0])])
        for ii in range(len(hidden_size)-1):
            self.linear_list.append(nn.Linear(hidden_size[ii], hidden_size[ii + 1]))
            self.linear_list.append(nn.LayerNorm(hidden_size[ii + 1]))
        self.relu = nn.ReLU()

    def forward(self, x, kge):
        x = torch.cat((x, kge), 1) if USE_KGE else x
        #print('MLPGenerator x:', x)
        for index, lin in enumerate(self.linear_list):
            x = lin(x)
            #print('MLPGenerator x:', index, x)
        x = self.relu(x)
        #x = x/torch.norm(x, dim=1, keepdim=True) if bool((torch.norm(x, dim=1, keepdim=True) > 0).all()) else x
        return x

class TCNDiscriminator(nn.Module):
    def __init__(self, input_size=1, num_clusters=6, num_channels=None, condition_size=32, condition_squeeze_size=4, dropout=0.3):
        super(TCNDiscriminator, self).__init__()
        if num_channels is None:
            num_channels = [1, 2, 4, 8, num_clusters, num_clusters]
        self.tcn_d = TemporalConvNet(input_size, num_channels, kernel_size=48, dropout=dropout)
        self.tcn_w = TemporalConvNet(input_size, num_channels, kernel_size=48*6, dropout=dropout)
        #self.tcn_m = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size[2], dropout=dropout)
        self.linear = nn.Linear(num_channels[-1]*2+condition_squeeze_size, num_clusters+1) if USE_KGE else \
                      nn.Linear(num_channels[-1]*2, num_clusters+1)
        self.linear_kge = nn.Linear(condition_size, condition_squeeze_size)
        self.softmax = nn.Softmax(dim=1)
        self.init_weights()

    def init_weights(self):
        self.linear.weight.data.normal_(0, 0.01)
        self.linear_kge.weight.data.normal_(0, 0.01)

    def forward(self, x, kge):
        x = x.unsqueeze(1)
        #print('TCNDiscriminator x:', x)
        y_d = self.tcn_d(x)[:,:,-1]
        #print('TCNDiscriminator y_d:', y_d)
        y_w = self.tcn_w(x)[:,:,-1]
        #print('TCNDiscriminator y_w:', y_w)
        #y_m = self.tcn_m(x)[:,:,-1]
        #print(y_d.shape, kge.shape)
        y = self.linear(torch.cat((y_d, y_w, self.linear_kge(kge)), 1)) if USE_KGE else \
            self.linear(torch.cat((y_d, y_w), 1))
        #print('TCNDiscriminator y:', y)
        #print(self.softmax(y))
        if self.linear.weight.shape[0] >= self.linear.weight.shape[1]:
            weight_mm = torch.mm(self.linear.weight.transpose(1, 0), self.linear.weight)
        else:
            weight_mm = torch.mm(self.linear.weight, self.linear.weight.transpose(1, 0))
        return self.softmax(y), weight_mm


class PositionalEncodingCat(nn.Module):
    def __init__(self, d_model, dropout, max_len=5000):
        """
        :param d_model: pe编码维度，一般与word embedding相同，方便相加
        :param dropout: dorp out
        :param max_len: 语料库中最长句子的长度，即word embedding中的L
        """
        super(PositionalEncodingCat, self).__init__()
        # 定义drop out
        self.dropout = nn.Dropout(p=dropout)

        # 计算pe编码
        pe = torch.zeros(max_len, d_model)  # 建立空表，每行代表一个词的位置，每列代表一个编码位
        position = torch.arange(0., max_len).unsqueeze(1)  # 建个arrange表示词的位置以便公式计算，size=(max_len,1)
        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))  # 计算公式中10000**（2i/d_model)

        pe[:, 0::2] = torch.sin(position * div_term)  # 计算偶数维度的pe值
        pe[:, 1::2] = torch.cos(position * div_term)  # 计算奇数维度的pe值
        pe = pe.unsqueeze(0)  # size=(1, L, d_model)，为了后续与word_embedding相加,意为batch维度下的操作相同
        self.register_buffer('pe', pe)  # pe值是不参加训练的

    def forward(self, x):
        # 输入的最终编码 = cat(word_embedding, positional_embedding)
        # x = x.permute(1,0,2)
        # print(x.shape, x.size(1), Variable(self.pe[:, :x.size(1)],requires_grad=False).shape)
        x = torch.cat((x, Variable(self.pe[:, :x.size(1)], requires_grad=False).expand(x.shape)),
                      2)  # size = [batch, L, d_model*2]
        # x = x + Variable(self.pe[:, :x.size(1)],requires_grad=False) #size = [batch, L, d_model]
        return x  # self.dropout(x)#(x.permute(1,0,2)) # size = [L, batch, d_model]

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout, max_len=5000):
        """
        :param d_model: pe编码维度，一般与word embedding相同，方便相加
        :param dropout: dorp out
        :param max_len: 语料库中最长句子的长度，即word embedding中的L
        """
        super(PositionalEncoding, self).__init__()
        # 定义drop out
        self.dropout = nn.Dropout(p=dropout)

        # 计算pe编码
        pe = torch.zeros(max_len, d_model)  # 建立空表，每行代表一个词的位置，每列代表一个编码位
        position = torch.arange(0., max_len).unsqueeze(1)  # 建个arrange表示词的位置以便公式计算，size=(max_len,1)
        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))  # 计算公式中10000**（2i/d_model)

        pe[:, 0::2] = torch.sin(position * div_term)  # 计算偶数维度的pe值
        pe[:, 1::2] = torch.cos(position * div_term)  # 计算奇数维度的pe值
        pe = pe.unsqueeze(0)  # size=(1, L, d_model)，为了后续与word_embedding相加,意为batch维度下的操作相同
        self.register_buffer('pe', pe)  # pe值是不参加训练的

    def forward(self, x):
        # 输入的最终编码 = cat(word_embedding, positional_embedding)
        # x = x.permute(1,0,2)
        # print(x.shape, x.size(1), Variable(self.pe[:, :x.size(1)],requires_grad=False).shape)
        # x = torch.cat((x, Variable(self.pe[:, :x.size(1)], requires_grad=False).expand(x.shape)),
        #              2)  # size = [batch, L, d_model*2]
        x = x + Variable(self.pe[:, :x.size(1)],requires_grad=False) #size = [batch, L, d_model]
        return self.dropout(x)#(x.permute(1,0,2)) # size = [L, batch, d_model]

class BiLSTMGenerator(nn.Module):
    def __init__(self, input_size=None, noise_size=32, out_seq_size=288, hidden_size=24, condition_size=32, dropout=0.3,
                 num_layers=3, bidirectional=True, head_num=1):
        super(BiLSTMGenerator, self).__init__()
        if input_size is None:
            input_size = hidden_size
        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True,
                           dropout=dropout, bidirectional=bidirectional)#, proj_size=int(input_size/2))

        self.linear_c0 = nn.Linear(condition_size, hidden_size * num_layers * 2)
        self.norm_c0 = nn.LayerNorm(hidden_size * num_layers * 2)

        self.linear_h0 = nn.Linear(noise_size, hidden_size * num_layers * 2)
        self.norm_h0 = nn.LayerNorm(hidden_size * num_layers * 2)

        self.linear_x = nn.Linear(noise_size + condition_size, out_seq_size) if USE_KGE else nn.Linear(noise_size,
                                                                                                       out_seq_size)
        self.norm_x = nn.LayerNorm(out_seq_size)
        self.pe = PositionalEncoding(1, dropout)

        self.query_linear = nn.Linear(hidden_size*(int(bidirectional)+1), hidden_size*(int(bidirectional)+1))
        self.key_linear = nn.Linear(hidden_size*(int(bidirectional)+1), hidden_size*(int(bidirectional)+1))
        self.value_linear = nn.Linear(hidden_size*(int(bidirectional)+1), hidden_size*(int(bidirectional)+1))
        self.attn = nn.MultiheadAttention(hidden_size*(int(bidirectional)+1), num_heads=head_num, batch_first=True)

        self.linear_out = nn.Linear(hidden_size*(int(bidirectional)+1), input_size)#int(input_size / 2))
        self.norm_out = nn.LayerNorm(input_size)#(int(input_size / 2))

        self.hidden_size = hidden_size
        self.out_seq_size = out_seq_size
        self.num_layers = num_layers
        self.input_size = input_size
        self.bidirectional = bidirectional

        self.relu = nn.ReLU()

    def forward(self, x, kge):
        batch_size = x.shape[0]
        c0 = self.norm_c0(self.linear_c0(kge)).reshape(batch_size, self.num_layers * 2, self.hidden_size).permute(1, 0, 2).contiguous() \
            if USE_KGE else \
            self.norm_c0(self.linear_c0(torch.zeros_like(kge))).reshape(
                batch_size, self.num_layers * 2, self.hidden_size).permute(1, 0,2).contiguous()
        h0 = self.norm_h0(self.linear_h0(x)).reshape(batch_size, self.num_layers * 2, self.hidden_size).permute(1, 0, 2).contiguous()
        x = torch.cat((x, kge), 1) if USE_KGE else x
        x = self.norm_x(self.linear_x(x)).unsqueeze(2)
        x = self.pe(x).reshape(batch_size, int(self.out_seq_size/self.input_size), self.input_size)
        x, (hn, cn) = self.rnn(x, (h0, c0))
        query_proj = self.query_linear(x)
        key_proj = self.key_linear(x)
        value_proj = self.value_linear(x)
        x, attn_output_weights = self.attn(query_proj, key_proj, value_proj)
        x = self.norm_out(self.linear_out(x)).reshape(batch_size, self.out_seq_size)
        x = self.relu(x)
        #x = x/torch.norm(x, dim=1, keepdim=True) if bool((torch.norm(x, dim=1, keepdim=True) > 0).all()) else x
        return x

class PatternGenerator(nn.Module):
    def __init__(self, pattern_num=6, input_size=None, noise_size=32, out_seq_size=288, hidden_size=24, condition_size=32, dropout=0.3,
                 num_layers=3, bidirectional=True, head_num=1):
        super(PatternGenerator, self).__init__()
        self.pattern_num = pattern_num
        self.generators = nn.ModuleList([])
        for ii in range(pattern_num):
            self.generators.append(BiLSTMGenerator(input_size=input_size,
                                                   noise_size=noise_size,
                                                   out_seq_size=out_seq_size,
                                                   hidden_size=hidden_size,
                                                   condition_size=condition_size,
                                                   dropout=dropout,
                                                   num_layers=num_layers,
                                                   bidirectional=bidirectional,
                                                   head_num=head_num)
                                   )
    def forward(self, x, kge):
        x = torch.cat(
            [self.generators[ii](x, kge).unsqueeze(1) for ii in range(self.pattern_num)], 1
        )
        return x

class LSTMSwitchGenerator(nn.Module):
    def __init__(self, pattern_num=6, input_size=None, noise_size=32, out_seq_size=288, hidden_size=24, condition_size=32, dropout=0.3,
                 num_layers=3, bidirectional=True, head_num=1):
        super(LSTMSwitchGenerator, self).__init__()
        if input_size is None:
            input_size = hidden_size
        input_size = input_size * pattern_num
        hidden_size = hidden_size * pattern_num
        out_seq_size = out_seq_size * pattern_num
        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True,
                           dropout=dropout, bidirectional=bidirectional)#, proj_size=int(input_size/2))

        self.linear_c0 = nn.Linear(condition_size, hidden_size * num_layers * (int(bidirectional)+1))
        self.norm_c0 = nn.LayerNorm(hidden_size * num_layers * (int(bidirectional)+1))

        self.linear_h0 = nn.Linear(noise_size, hidden_size * num_layers * (int(bidirectional)+1))
        self.norm_h0 = nn.LayerNorm(hidden_size * num_layers * (int(bidirectional)+1))

        self.linear_x = nn.Linear(noise_size + condition_size, out_seq_size) if USE_KGE\
            else nn.Linear(noise_size, out_seq_size)
        self.norm_x = nn.LayerNorm(out_seq_size)
        self.pe = PositionalEncoding(1, dropout)

        self.query_linear = nn.Linear(hidden_size*(int(bidirectional)+1), hidden_size*(int(bidirectional)+1))
        self.key_linear = nn.Linear(hidden_size*(int(bidirectional)+1), hidden_size*(int(bidirectional)+1))
        self.value_linear = nn.Linear(hidden_size*(int(bidirectional)+1), hidden_size*(int(bidirectional)+1))
        self.attn = nn.MultiheadAttention(hidden_size*(int(bidirectional)+1), num_heads=head_num, batch_first=True)

        self.linear_out = nn.Linear(hidden_size*(int(bidirectional)+1), input_size)#int(input_size / 2))
        self.norm_out = nn.LayerNorm(input_size)#(int(input_size / 2))

        self.pattern_num = pattern_num
        self.hidden_size = hidden_size
        self.out_seq_size = out_seq_size
        self.num_layers = num_layers
        self.input_size = input_size
        self.bidirectional = bidirectional

        self.relu = nn.ReLU()

        #self.gumbel_softmax = F.gumbel_softmax()

    def forward(self, x, kge):
        batch_size = x.shape[0]
        c0 = self.norm_c0(self.linear_c0(kge)).reshape(batch_size, self.num_layers * (int(self.bidirectional)+1), self.hidden_size).permute(1, 0, 2).contiguous() \
            if USE_KGE else torch.zeros(batch_size, self.num_layers * (int(self.bidirectional)+1), self.hidden_size).permute(1, 0, 2).contiguous()
        h0 = self.norm_h0(self.linear_h0(x)).reshape(batch_size, self.num_layers * (int(self.bidirectional)+1), self.hidden_size).permute(1, 0, 2).contiguous()
        x = torch.cat((x, kge), 1) if USE_KGE else x
        x = self.norm_x(self.linear_x(x)).unsqueeze(2)
        x = self.pe(x).reshape(batch_size, int(self.out_seq_size/self.input_size), self.input_size)
        x, (hn, cn) = self.rnn(x, (h0, c0))
        #query_proj = self.query_linear(x)
        #key_proj = self.key_linear(x)
        #value_proj = self.value_linear(x)
        #x, attn_output_weights = self.attn(query_proj, key_proj, value_proj)
        x = self.norm_out(self.linear_out(x)).reshape(batch_size, self.pattern_num, int(self.out_seq_size/self.pattern_num))
        #x = self.relu(x)
        x = F.gumbel_softmax(x, tau=0.1, hard=True, eps=1e-10, dim=1)
        #x = x/torch.norm(x, dim=1, keepdim=True) if bool((torch.norm(x, dim=1, keepdim=True) > 0).all()) else x
        return x

class MLPSwitchGenerator(nn.Module):
    def __init__(self, pattern_num=6, noise_size=32, out_seq_size=288, hidden_size=None, condition_size = 32):
        super(MLPSwitchGenerator, self).__init__()
        self.pattern_num = pattern_num
        self.out_seq_size = out_seq_size
        if hidden_size is None:
            hidden_size = [6 * 12, 6 * 48, 6 * 128, 6*256]
        hidden_size.append(out_seq_size * pattern_num)
        self.linear_in = nn.Linear(noise_size+condition_size, hidden_size[0]) if USE_KGE else \
                         nn.Linear(noise_size, hidden_size[0])
        self.linear_list = nn.ModuleList([self.linear_in, nn.LayerNorm(hidden_size[0])])
        for ii in range(len(hidden_size)-1):
            self.linear_list.append(nn.Linear(hidden_size[ii], hidden_size[ii + 1]))
            self.linear_list.append(nn.LayerNorm(hidden_size[ii + 1]))
        self.relu = nn.ReLU()

    def forward(self, x, kge):
        x = torch.cat((x, kge), 1) if USE_KGE else x
        #print('MLPGenerator x:', x)
        for index, lin in enumerate(self.linear_list):
            x = lin(x)
            #print('MLPGenerator x:', index, x)
        x = x.reshape(kge.shape[0], self.pattern_num, self.out_seq_size)
        x = F.gumbel_softmax(x, tau=0.1, hard=True, eps=1e-10, dim=1)
        #x = self.relu(x)
        #x = x/torch.norm(x, dim=1, keepdim=True) if bool((torch.norm(x, dim=1, keepdim=True) > 0).all()) else x
        return x

class SwitchGenerator(nn.Module):
    def __init__(self, pattern_num=6, input_size=None, noise_size=32, out_seq_size=288, hidden_size=1, condition_size=32, dropout=0.3,
                 num_layers=3, bidirectional=True):
        super(SwitchGenerator, self).__init__()
        if input_size is None:
            input_size = hidden_size
        out_seq_size = out_seq_size
        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True,
                           dropout=dropout, bidirectional=bidirectional)#, proj_size=proj_size)

        self.linear_c0 = nn.Linear(condition_size, hidden_size * num_layers * (int(bidirectional)+1))
        self.norm_c0 = nn.LayerNorm(hidden_size * num_layers * (int(bidirectional)+1))

        self.linear_h0 = nn.Linear(noise_size, hidden_size * num_layers * (int(bidirectional)+1))
        self.norm_h0 = nn.LayerNorm(hidden_size * num_layers * (int(bidirectional)+1))

        self.linear_x = nn.Linear(noise_size + condition_size, out_seq_size * pattern_num) if USE_KGE\
            else nn.Linear(noise_size, out_seq_size * pattern_num)
        self.norm_x = nn.LayerNorm(out_seq_size * pattern_num)


        self.linear_out = nn.Linear(hidden_size*(int(bidirectional)+1), input_size)#int(input_size / 2))
        self.norm_out = nn.LayerNorm(input_size)#(int(input_size / 2))

        self.pattern_num = pattern_num
        self.hidden_size = hidden_size
        self.out_seq_size = out_seq_size
        self.num_layers = num_layers
        self.input_size = input_size
        self.bidirectional = bidirectional

        self.relu = nn.ReLU()

        #self.gumbel_softmax = F.gumbel_softmax()

    def forward(self, x, kge):
        batch_size = x.shape[0]
        c0 = self.norm_c0(self.linear_c0(kge)).reshape(batch_size, self.num_layers * (int(self.bidirectional)+1), self.hidden_size).permute(1, 0, 2).contiguous() \
            if USE_KGE else torch.zeros(batch_size, self.num_layers * (int(self.bidirectional)+1), self.hidden_size).permute(1, 0, 2).contiguous()
        h0 = self.norm_h0(self.linear_h0(x)).reshape(batch_size, self.num_layers * (int(self.bidirectional)+1), self.hidden_size).permute(1, 0, 2).contiguous()
        x = torch.cat((x, kge), 1) if USE_KGE else x
        x = self.norm_x(self.linear_x(x)).unsqueeze(2)
        x, (hn, cn) = self.rnn(x, (h0, c0))
        x = self.norm_out(self.linear_out(x)).reshape(batch_size, self.pattern_num, self.out_seq_size)
        #x = self.relu(x)
        #x = F.gumbel_softmax(x, tau=0.1, hard=True, eps=1e-10, dim=1)
        #x = x/torch.norm(x, dim=1, keepdim=True) if bool((torch.norm(x, dim=1, keepdim=True) > 0).all()) else x
        return x

class RNNSwitchGenerator(nn.Module):
    def __init__(self, pattern_num=6, noise_size=32, out_seq_size=288, condition_size=32):
        super(RNNSwitchGenerator, self).__init__()
        self.linear_x0 = nn.Linear(condition_size, pattern_num)
        self.norm_x0 = nn.LayerNorm(pattern_num)

        self.linear_h0 = nn.Linear(noise_size, pattern_num)
        self.norm_h0 = nn.LayerNorm(pattern_num)

        self.linear_x = nn.Linear(pattern_num, pattern_num)
        self.linear_h = nn.Linear(pattern_num, pattern_num)
        self.norm = nn.LayerNorm(pattern_num)

        self.out_seq_size = out_seq_size
        self.pattern_num = pattern_num

    def forward(self, z, kge):
        batch_size = z.shape[0]
        x = self.linear_x0(kge) if USE_KGE else torch.zeros(batch_size, self.pattern_num)
        h = self.linear_h0(z)
        out = h.unsqueeze(2)

        for step in range(self.out_seq_size):
            h = self.norm(self.linear_h(h) + self.linear_x(x))
            x = F.gumbel_softmax(h, tau=0.1, hard=True, eps=1e-10, dim=1)
            out = torch.cat((out, x.unsqueeze(2)), 2)
        out = out[:, :, 1:]
        return out

class MarkovSwitchGenerator(nn.Module):
    def __init__(self, pattern_num=6, noise_size=32, out_seq_size=288, condition_size=32, tau=0.1):
        super(MarkovSwitchGenerator, self).__init__()
        self.linear_x0 = nn.Linear(condition_size+noise_size, pattern_num) if USE_KGE else nn.Linear(noise_size, pattern_num)
        self.norm_x0 = nn.LayerNorm(pattern_num)

        self.linear = nn.Linear(pattern_num, pattern_num)
        self.norm = nn.LayerNorm(pattern_num)

        self.relu = nn.ReLU()

        self.out_seq_size = out_seq_size
        self.pattern_num = pattern_num
        self.tau = tau

        self.init_weights()

    def init_weights(self):
        self.linear.weight = torch.nn.Parameter(torch.eye(self.pattern_num)+torch.rand(self.pattern_num,self.pattern_num)/10)

    def forward(self, x, kge):
        batch_size = x.shape[0]
        x = torch.cat((x, kge), 1) if USE_KGE else x
        p_0 = self.linear_x0(self.relu(x))
        x = F.gumbel_softmax(p_0, tau=0.1, hard=True, eps=1e-10, dim=1)
        out = x.unsqueeze(2)

        for t in range(self.out_seq_size-1):
            p_t = self.relu(self.norm(self.linear(x)))
            x = F.gumbel_softmax(p_t, tau=self.tau, hard=True, eps=1e-10, dim=1)
            out = torch.cat((out, x.unsqueeze(2)), 2)
        return out
